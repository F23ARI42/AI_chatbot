[
  {
    "input": "1. Weight Sharing and Local Spatial Coherence",
    "output": "CNNs take advantage of the spatial structure in data. Instead of learning separate parameters for each input feature, convolutional layers use a set of shared weights (kernels) that slide across the input image. This means the same feature detector is used at every position, drastically reducing the total number of parameters.\nBenefit: Significant reduction in computational cost, making CNNs efficient even on low-power GPUs or machines without dedicated graphics cards."
  },
  {
    "input": "2. Memory Efficiency",
    "output": "Because of weight sharing and the nature of convolutions, CNNs generally require far fewer parameters than fully connected neural networks.\nExample: On the MNIST dataset, a simple CNN with one hidden layer and 10 output nodes might use only a few hundred parameters, whereas a fully connected neural network with similar capacity could require upwards of 19,000 parameters for the same task.\nBenefit: Lower memory requirements make CNNs suitable for devices with limited resources and also reduce overfitting risk."
  },
  {
    "input": "3. Robustness to Local Variations",
    "output": "CNNs are specifically designed to extract features from different regions of an image, making them robust to small shifts and variations in the input. Example: If a fully connected network is trained for face recognition using only head-shot images, it may fail when presented with full-body images. However, a CNN can adapt to such changes and still recognize faces because it focuses on spatial features rather than exact positions."
  },
  {
    "input": "4. Equivariance to Transformations",
    "output": "Equivariance refers to the property where a transformation applied to the input results in the same transformation in the output. In CNNs, convolution operations are equivariant to translations: if an object shifts in the image, its feature map also shifts correspondingly.\nMathematical Formulation: If f is a convolution operation and g is a transformation (like translation), thenf(g(x))=g(f(x)).\nBenefit: This property helps the model reliably detect features even when they move within the input, increasing reliability and consistency in predictions."
  },
  {
    "input": "5. Independence from Image Transformations",
    "output": "CNNs exhibit relative invariance to common geometric transformations such as translation, rotation and scaling. This means they can recognize objects regardless of their orientation or size within the image.\nTranslation Invariance Example:A CNN can detect the same object even if it's shifted to different parts of the image.\nRotation Invariance Example: A CNN can still correctly identify an object if it is rotated, improving versatility for real-world applications."
  }
]