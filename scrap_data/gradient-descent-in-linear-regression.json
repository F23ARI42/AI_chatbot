[
  {
    "input": "Why Use Gradient Descent for Linear Regression?",
    "output": "Linear regressionfinds the best-fit line for a dataset by minimizing the error between the actual and predicted values. This error is measured using thecost functionusually Mean Squared Error (MSE). The goal is to find the model parameters i.e. the slope m and the intercept b that minimize this cost function.\nFor simple linear regression, we can use formulas likeNormal Equationto find parameters directly. However for large datasets or high-dimensional data these methods become computationally expensive due to:\nLarge matrix computations.\nMemory limitations.\nIn models likepolynomial regression, the cost function becomes highly complex and non-linear, so analytical solutions are not available. That’s wheregradient descentplays an important role even for:\nLarge datasets.\nComplex, high-dimensional problems."
  },
  {
    "input": "How Does Gradient Descent Work in Linear Regression?",
    "output": "Lets see various steps involved in the working of Gradient Descent in Linear Regression:\n1. Initializing Parameters: Start with random initial values for the slope (m) and intercept (b).\n2. Calculate the Cost Function: Measure the error using theMean Squared Error (MSE):\n3. Compute the Gradient: Calculate how much the cost function changes with respect tomandb.\nFor slopem:\nFor interceptb:\n4. Update Parameters: Changemandbto reduce the error:\nFor slopem:\nFor interceptb:\nHere\\alphais the learning rate that controls the size of each update.\n5. Repeat: Keep repeating steps 2–4 until the error stops decreasing significantly."
  },
  {
    "input": "Implementation of Gradient Descent in Linear Regression",
    "output": "Let’s implement linear regression step by step. To understand how gradient descent improves the model, we will first build a simple linear regression without using gradient descent and observe its results.\nHere we will be usingNumpy,Pandas,MatplotlibandSckit learnlibraries for this.\nX, y = make_regression(n_samples=100, n_features=1, noise=15, random_state=42): Generating 100 data points with one feature and some noise for realism.\nX_b = np.c_[np.ones((m, 1)), X]: Addind a column of ones to X to account for the intercept term in the model.\ntheta = np.array([[2.0], [3.0]]): Initializing model parameters (intercept and slope) with starting values.\nOutput:\nHere the model’s predictions are not accurate and the line does not fit the data well. This happens because the initial parameters are not optimized which prevents the model from finding the best-fit line.\nNow we will applygradient descentto improve the model and optimize these parameters.\nlearning_rate = 0.1, n_iterations = 100:Set the learning rate and number of iterations for gradient descent to run respectively.\ngradients = (2 / m) * X_b.T.dot(y_pred - y): Finding gradients of the cost function with respect to parameters.\ntheta -= learning_rate * gradients: Updating parameters by moving opposite to the gradient direction.\nOutput:\nLinear Regression with Gradient Descent shows how the model gradually learns to fit the line that minimizes the difference between predicted and actual values by updating parameters step by step."
  }
]