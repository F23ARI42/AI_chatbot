[
  {
    "input": "SMOTE (Synthetic Minority Oversampling Technique) - Oversampling",
    "output": "SMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them. SMOTE synthesises new minority instances between existing minority instances. It generates the\nvirtual training records by linear interpolation\nfor the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.\nMore Deep Insights of how SMOTE Algorithm work !"
  },
  {
    "input": "NearMiss Algorithm - Undersampling",
    "output": "NearMiss is an under-sampling technique. It aims to balance class distribution by randomly eliminating majority class examples. When instances of two different classes are very close to each other, we remove the instances of the majority class to increase the spaces between the two classes. This helps in the classification process.  To prevent problem of\ninformation loss\nin most under-sampling techniques,\nnear-neighbor\nmethods are widely used.\nThe basic intuition about the working of near-neighbor methods is as follows:\nFor finding n closest instances in the majority class, there are several variations of applying NearMiss Algorithm :\nThis article helps in better understanding and hands-on practice on how to choose best between different imbalanced data handling techniques."
  },
  {
    "input": "Load libraries and data file",
    "output": "The dataset consists of transactions made by credit cards. This dataset has\n492 fraud transactions out of 284, 807 transactions\n. That makes it highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nOutput:\nOutput:"
  },
  {
    "input": "Split the data into test and train sets",
    "output": "Output:"
  },
  {
    "input": "Now train the model without handling the imbalanced class distribution",
    "output": "Output:\nThe accuracy comes out to be 100% but did you notice something strange ?\nThe recall of the minority class in very less. It proves that the model is more biased towards majority class. So, it proves that this is not the best model.  Now, we will apply different\nimbalanced data handling techniques\nand see their accuracy and recall results."
  },
  {
    "input": "Using SMOTE Algorithm",
    "output": "Output:\nLook!\nthat SMOTE Algorithm has oversampled the minority instances and made it equal to majority class. Both categories have equal amount of records. More specifically, the minority class has been increased to the total number of majority class. Now see the accuracy and recall results after applying SMOTE algorithm (Oversampling).\nOutput:\nWow\n, We have reduced the accuracy to 98% as compared to previous model but the recall value of minority class has also improved to 92 %. This is a good model compared to the previous one. Recall is great. Now, we will apply NearMiss technique to Under-sample the majority class and see its accuracy and recall results."
  },
  {
    "input": "NearMiss Algorithm:",
    "output": "Output:\nThe\nNearMiss Algorithm\nhas undersampled the majority instances and made it equal to majority class. Here, the majority class has been reduced to the total number of minority class, so that both classes will have equal number of records.\nOutput:\nThis model is better than the first model because it classifies better and also the recall value of minority class is 95 %. But due to undersampling of majority class, its recall has decreased to 56 %. So in this case, SMOTE is giving me a great accuracy and recall, Iâ€™ll go ahead and use that model! :)"
  }
]