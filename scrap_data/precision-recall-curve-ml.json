[
  {
    "input": "Key Concepts of Precision and Recall",
    "output": "Before understanding the PR curve let’s understand:"
  },
  {
    "input": "1. Precision",
    "output": "It refers to the proportion of correct positive predictions (True Positives) out of all the positive predictions made by the model i.e True Positives + False Positives. It is a measure of the accuracy of the positive predictions. The formula for Precision is:\n\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\nA high Precision means that the model makes few False Positives. This metric is especially useful when the cost of false positives is high such as email spam detection."
  },
  {
    "input": "2. Recall",
    "output": "It is also known as Sensitivity or True Positive Rate where we measures the proportion of actual positive instances that were correctly identified by the model. It is the ratio of True Positives to the total actual positives i.e True Positives + False Negatives. The formula for Recall is:\n\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\nA high Recall means the model correctly identifies most of the positive instances which is critical when False Negatives are costly like in medical diagnoses."
  },
  {
    "input": "3. Confusion Matrix",
    "output": "To better understand Precision and Recall we can use aConfusion Matrixwhich summarizes the performance of a classifier in four essential terms:\nTrue Positives (TP): Correctly predicted positive instances.\nFalse Positives (FP): Incorrectly predicted positive instances.\nTrue Negatives (TN): Correctly predicted negative instances.\nFalse Negatives (FN): Incorrectly predicted negative instances."
  },
  {
    "input": "How Precision-Recall Curve Works",
    "output": "The PR curve is created by changing the decision threshold of your model and checking how the precision and recall change at each step. The threshold is the cutoff point where you decide:\nIf the probability is above the threshold you predict positive.\nIf it's below you predict negative.\nBy default this threshold is usually 0.5 but you can move it up or down.\nA PR curve is useful when dealing with imbalanced datasets where one class significantly outnumbers the other. In such cases the ROC curve might show overly optimistic results as it doesn’t account for class imbalance as effectively as the Precision-Recall curve. The figure below shows a comparison of sample PR and ROC curves. It is desired that the algorithm should have both high precision and high recall. However most machine learning algorithms often involve a trade-off between the two. A good PR curve has greaterAUC (area under the curve).\nIn the figure above the classifier corresponding to the blue line has better performance than the classifier corresponding to the green line. It is important to note that the classifier that has a higher AUC on theROC curvewill always have a higher AUC on the PR curve as well."
  },
  {
    "input": "Interpreting a Precision-Recall Curve",
    "output": "Consider an algorithm that classifies whether or not a document belongs to the category \"Sports\" news. Assume there are 12 documents with the following ground truth (actual) and classifier output class labels.\nNow let us find TP, TN, FP and FN values.\nTrue Positives (TP):Documents that were accurately categorised as \"Sports\" and that were in fact about sports. Documents D1, D2, D10 and D11 in this scenario are instances of TP.\nTrue Negatives (TN):True Negatives are those cases in which the document was appropriately labelled as \"Not sports\" even though it had nothing to do with sports. In this instance TN is demonstrated by documents D5, D8 and D9.\nFalse Positives (FP):Documents that were mistakenly categorised as \"Sports\" even though they had nothing to do with sports. Here are some FP examples documents D3 and D7.\nFalse Negatives (FN):Examples of documents that were mistakenly labelled as \"Not sports\" but in reality they were about sports. Documents D4, D6 and D12 in this case are FN examples.\nGiven these counts:TP=4,TN=3,FP=2,FN=3\nFinally precision and recall are calculated as follows:\n\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{4}{6} = \\frac{2}{3}\n\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{4}{7}\nIt follows that the recall is 4/7 when the precision is 2/3. All the cases that were anticipated to be positive, two-thirds were accurately classified (precision) and of all the instances that were actually positive, the model was able to capture four-sevenths of them (recall)."
  },
  {
    "input": "When to use PR curve and ROC curve",
    "output": "Choosing between ROC and Precision-Recall depends on the specific needs of the problem, understanding data distribution and the consequences of different types of errors.\nThe PR curve helps us visualize how well our model is performing across various thresholds. It provides insights into how changes in decision thresholds affect Precision and Recall. For example increasing the threshold might increase Precision i.e fewer False Positives but it could lower Recall i.e more False Negatives. The goal is to find a balance that best suits the specific needs of your application whether it’s minimizing False Positives or False Negatives.\nROC curves are suitable when the class distribution is balanced and false positives and false negatives have similar consequences. They show the trade-off between sensitivity and specificity."
  },
  {
    "input": "Step 1: Importing Necessary Libraries",
    "output": "We will generate precision-recall curve withscikit-learnand visualize the results withMatplotlib."
  },
  {
    "input": "Step 2: Dataset Used",
    "output": "This code generates a synthetic dataset for a binary classification problem using scikit-learn's make_classification function."
  },
  {
    "input": "Step 3: Train and Test Split",
    "output": "The train_test_split function in scikit-learn is used to split dataset into training and testing sets."
  },
  {
    "input": "Step 4: Model Building",
    "output": "Here we are usingLogistic Regressionto train the model on the training data set. A popular algorithm forbinary classification. It is implemented by the scikit-learn class LogisticRegression."
  },
  {
    "input": "Step 5: Model Prediction",
    "output": "We will calculate Precision and Recall which will be used to draw a precision-recall curve and then we will calculate AUC for the precision-recall curve."
  },
  {
    "input": "Step 6: Plotting PR Curve",
    "output": "It visualizes the precision-recall curve and evaluate the precision vs recall trade-off at various decision thresholds.\nOutput:\nThis curve shows the trade-off between Precision and Recall across different decision thresholds. The Area Under the Curve (AUC) is 0.94 suggesting that the model performs well in balancing both Precision and Recall. A higher AUC typically indicates better model performance."
  }
]