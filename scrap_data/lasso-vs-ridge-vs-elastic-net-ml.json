[
  {
    "input": "Ridge Regression (L2 Regularization)",
    "output": "Ridge regressionis a technique used to address overfitting by adding a penalty to the model's complexity. It introduces an L2 penalty (also called L2 regularization) which is the sum of the squares of the model's coefficients. This penalty term reduces the size of large coefficients but keeps all features in the model. This prevents overfitting with correlated features.\nFormula for Ridge Regression:\nwhere:\nThe first term calculates the prediction error.\nThe second term penalizes large coefficients controlled by\\lambda.\nExample: Let’s assume we are predicting house prices with features like size, location and number of rooms. The model might give coefficients like:\n\\beta1= 5 (Size coefficient)\n\\beta2= 3 (Number of rooms coefficient)\n\\lambda= 0.1 (regularization strength).\nThe penalty term for Ridge would be calculated as:\n\\lambda \\left( \\beta_1^2 + \\beta_2^2 \\right) = 0.1 \\cdot \\left( 5^2 + 3^2 \\right) = 0.1 \\cdot \\left( 25 + 9 \\right) = 0.1 \\cdot 34 = 3.4\nThis penalty shrinks the coefficients to reduce overfitting but does not remove any features."
  },
  {
    "input": "Lasso Regression (L1 Regularization)",
    "output": "Lasso regressionaddresses overfitting by adding an L1 penalty i.e sum of absolute coefficients to the model's loss function. This encourages some coefficients to become exactly zero helps in effectively removing less important features. It also helps to simplify the model by selecting only the key features.\nFormula for Lasso Regression:\nwhere:\nThe first term calculates the prediction error.\nThe second term encourages sparsity by shrinking some coefficients to zero.\nExample: Let’s assume the same house price prediction example but now using Lasso. Assume:\n\\beta1= 5 (Size coefficient)\n\\beta2= 0 (Number of rooms coefficient is irrelevant and should be removed)\n\\lambda= 0.1 (regularization strength).\nThe penalty term for Lasso would be:\n\\lambda \\cdot |\\beta_1| = 0.1 \\cdot |5| = 0.1 \\cdot 5 = 0.5\nHere Lasso forces\\beta2= 0 removing the Number of Rooms feature entirely from the model."
  },
  {
    "input": "Elastic Net Regression (L1 + L2 Regularization)",
    "output": "Elastic Net regressioncombines both L1 (Lasso) and L2 (Ridge) penalties to perform feature selection, manage multicollinearity and balancing coefficient shrinkage. This works well when there are many correlated features helps in avoiding the problem where Lasso might randomly pick one and ignore others.\nFormula for Elastic Net Regression:\nwhere:\nThe first term calculates the prediction error.\nThe second term applies the L1 penalty for feature selection.\nThe third term applies the L2 penalty to handle multicollinearity.\nIt provides a more stable and generalizable model compared to using Lasso or Ridge alone.\nExample: Let’s assume we are predicting house prices using Size and Number of Rooms. Assume:\n\\beta1= 5 (Size coefficient)\n\\beta2= 3 (Number of rooms coefficient)\n\\lambda1= 0.1 (L1 regularization).\n\\lambda2= 0.1 (L2 regularization).\nThe penalty term for Elastic Net would be:\n\\lambda_1 \\cdot (|\\beta_1| + |\\beta_2|) + \\lambda_2 \\cdot (\\beta_1^2 + \\beta_2^2) = 0.1 \\cdot (|5| + |3|) + 0.1 \\cdot (5^2 + 3^2) = 0.1 \\cdot (5 + 3) + 0.1 \\cdot (25 + 9) = 0.1 \\cdot 8 + 0.1 \\cdot 34 = 0.8 + 3.4 = 4.2\nThis penalty shrinks both coefficients but because of the mixture of L1 and L2 it does not force any feature to zero unless absolutely necessary."
  },
  {
    "input": "Lasso vs Ridge vs Elastic Net",
    "output": "Now lets see a tabular comparison between these three for better understanding.\nUsing the right regularization technique helps us to build models that are both accurate and easy to interpret."
  }
]