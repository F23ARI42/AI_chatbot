[
  {
    "input": "Spectral Clustering",
    "output": "Spectral Clustering is a variant of the clustering algorithm that uses the connectivity between the data points to form the clustering. It uses eigenvalues and eigenvectors of the data matrix to forecast the data into lower dimensions space to cluster the data points. It is based on the idea of a graph representation of data where the data point are represented as nodes and the similarity between the data points are represented by an edge."
  },
  {
    "input": "Steps performed for spectral Clustering",
    "output": "Building the Similarity Graph Of The Data:This step builds the Similarity Graph in the form of an adjacency matrix which is represented by A. The adjacency matrix can be built in the following manners:\nEpsilon-neighbourhood Graph:A parameter epsilon is fixed beforehand. Then, each point is connected to all the points which lie in its epsilon-radius. If all the distances between any two points are similar in scale then typically the weights of the edges ie the distance between the two points are not stored since they do not provide any additional information. Thus, in this case, the graph built is an undirected and unweighted graph.\nK-Nearest NeighboursA parameter k is fixed beforehand. Then, for two vertices u and v, an edge is directed from u to v only if v is among the k-nearest neighbours of u. Note that this leads to the formation of a weighted and directed graph because it is not always the case that for each u having v as one of the k-nearest neighbours, it will be the same case for v having u among its k-nearest neighbours. To make this graph undirected, one of the following approaches is followed:-Direct an edge from u to v and from v to u if either v is among the k-nearest neighbours of uORu is among the k-nearest neighbours of v.Direct an edge from u to v and from v to u if v is among the k-nearest neighbours of uANDu is among the k-nearest neighbours of v.Fully-Connected Graph:To build this graph, each point is connected with an undirected edge-weighted by the distance between the two points to every other point. Since this approach is used to model the local neighbourhood relationships thus typically the Gaussian similarity metric is used to calculate the distance.\nDirect an edge from u to v and from v to u if either v is among the k-nearest neighbours of uORu is among the k-nearest neighbours of v.\nDirect an edge from u to v and from v to u if v is among the k-nearest neighbours of uANDu is among the k-nearest neighbours of v.\nFully-Connected Graph:To build this graph, each point is connected with an undirected edge-weighted by the distance between the two points to every other point. Since this approach is used to model the local neighbourhood relationships thus typically the Gaussian similarity metric is used to calculate the distance.\nProjecting the data onto a lower Dimensional Space:This step is done to account for the possibility that members of the same cluster may be far away in the given dimensional space. Thus the dimensional space is reduced so that those points are closer in the reduced dimensional space and thus can be clustered together by a traditional clustering algorithm. It is done by computing theGraph Laplacian Matrix.\nTo compute it though first, the degree of a node needs to be defined. The degree of the ith node is given byd_{i} = \\sum _{j=1|(i, j)\\epsilon E}^{n} w_{ij}Note thatw_{ij}is the edge between the nodes i and j as defined in the adjacency matrix above.\nThe degree matrix is defined as follows:-D_{ij} = \\left\\{\\begin{matrix} d_{i}, i=j & \\\\0, i\\neq j & \\end{matrix}\\right.\nThus the Graph Laplacian Matrix is defined as:-L = D-A\nThis Matrix is then normalized for mathematical efficiency. To reduce the dimensions, first, the eigenvalues and the respective eigenvectors are calculated. If the number of clusters is k then the first eigenvalues and their eigenvectors are taken and stacked into a matrix such that the eigenvectors are the columns.\nCode For Calculating eigenvalues and eigenvector of the matrix in Python\n\nClustering the Data:This process mainly involves clustering the reduced data by using any traditional clustering technique - typically K-Means Clustering. First, each node is assigned a row of the normalized of the Graph Laplacian Matrix. Then this data is clustered using any traditional technique. To transform the clustering result, the node identifier is retained.\nProperties:"
  },
  {
    "input": "Credit Card Data Clustering Using Spectral Clustering",
    "output": "The below steps demonstrate how to implement Spectral Clustering using Sklearn. The data for the following steps is theCredit Card Datawhich can be downloaded from Kaggle\nStep 1: Importing the required libraries\nWe will first import all the libraries that are needed for this project\nStep 2: Loading and Cleaning the Data\nOutput:\nStep 3: Preprocessing the data to make the data visualizable\n\nStep 4: Building the Clustering models and Visualizing the Clustering\nIn the below steps, two different Spectral Clustering models with different values for the parameter 'affinity'. You can read about the documentation of the Spectral Clustering classhere. a)affinity = 'rbf'\nOutput:\n\nb)affinity = 'nearest_neighbors'\nOutput:\n\nStep 5: Evaluating the performances\n\nStep 6: Comparing the performances\nOutput:\n\nSpectral Clustering is a type of clustering algorithm in machine learning that uses eigenvectors of a similarity matrix to divide a set of data points into clusters. The basic idea behind spectral clustering is to use the eigenvectors of the Laplacian matrix of a graph to represent the data points and find clusters by applying k-means or another clustering algorithm to the eigenvectors."
  }
]