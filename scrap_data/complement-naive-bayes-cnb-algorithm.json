[
  {
    "input": "Challenge of Unbalanced Datasets",
    "output": "An unbalanced dataset means one type of data appears much more often than the other. This often happens in spam filtering (more normal emails than spam) or medical diagnosis (more healthy cases than disease cases).\nExample:"
  },
  {
    "input": "Formula",
    "output": "For a class c and feature f:\ncount(f, \\bar{c})= count of feature f in the complement of class c\n\\alpha= smoothing parameter (Laplace smoothing)\n|V|= vocabulary size"
  },
  {
    "input": "Example",
    "output": "Suppose classifying sentences as Apples or Bananas using word frequencies, To classify a new sentence (Round=1, Red=1, Soft=1):\nMNB would estimate probabilities for Apples using only Apples data\nCNB estimates probabilities for Apples using Bananas' data (complement) and vice versa\nSolving by CNB:We classify a new sentence with features {Round =1, Red =1, Soft =1} and vocabulary {Round, Red, Soft}.\nStep 1:Complement counts\nFor Apples, use Bananas’ counts -> {Round:5, Red:1, Soft:3}\nFor Bananas, use Apples’ counts -> {Round:3, Red:4, Soft:1}\nStep 2:Probabilities (using Laplace smoothing, α =1)\nFor Apples:\nRound = (5+1)/(5+1+3+3) = 6/12 = 0.5\nRed   = (1+1)/12 = 0.167\nSoft  = (3+1)/12 = 0.333\nFor Bananas:\nRound = (3+1)/(3+1+4+1) = 4/11 ≈ 0.364\nRed   = (4+1)/11 = 0.455\nSoft  = (1+1)/11 = 0.182\nStep 3:Scores, Multiply feature probabilities:\nApples = 0.5 × 0.167 × 0.333 ≈ 0.0278\nBananas = 0.364 × 0.455 × 0.182 ≈ 0.0301\nFinal Result -> Bananas"
  },
  {
    "input": "Implementing CNB",
    "output": "We can implement CNB using scikit-learn on the wine dataset (for demonstration purposes)."
  },
  {
    "input": "1. Import libraries and load data",
    "output": "We will import and load the required libraries\nImport load_wine for dataset loading from sklearn.\nUse train_test_split to divide data into training and test sets.\nImport ComplementNB as the classifier.\nImport evaluation metrics: classification_report and accuracy_score."
  },
  {
    "input": "2. Split into training and test sets",
    "output": "We will split the dataset into training and test sets:\nSplit the dataset into 70% training and 30% testing data.\nSet random_state=42 for reproducibility."
  },
  {
    "input": "3. Train the CNB classifier",
    "output": "We will train the Complement Naive Bayes classifier\nCreate a ComplementNB instance.\nFit the classifier on the training data."
  },
  {
    "input": "4. Evaluate the model",
    "output": "We will now evaluate the trained model:\nPredict class labels for the test set using predict().\nPrint the accuracy score and the classification report for detailed metrics."
  },
  {
    "input": "Limitations of CNB",
    "output": "Feature independence assumption: Like all Naive Bayes variants, CNB assumes that features are conditionally independent given the class. This assumption is rarely true in real-world datasets and can reduce accuracy when violated.\nBest suited for discrete features: CNB is primarily designed for tasks with discrete data, such as word counts in text classification. Continuous data typically requires preprocessing for optimal results.\nBias in balanced datasets: The complement-based parameter estimation can introduce unnecessary bias when classes are already balanced. This may reduce its advantage compared to standard Naive Bayes models."
  },
  {
    "input": "Related articles",
    "output": "Naive Bayes Classifiers\nGaussian Naive Bayes\nMultinomial Naive Bayes"
  }
]