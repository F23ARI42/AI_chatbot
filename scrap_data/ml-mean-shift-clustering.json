[
  {
    "input": "Kernel Density Estimation -",
    "output": "The first step when applying mean shift clustering algorithms is representing your data in a mathematical manner this means representing your data as points such as the set below.\nMean-shift builds upon the concept of kernel density estimation, in short KDE. Imagine that the above data was sampled from a probability distribution. KDE is a method to estimate the underlying distribution also called the probability density function for a set of data. It works by placing a kernel on each point in the data set. A kernel is a fancy mathematical word for a weighting function generally used in convolution. There are many different types of kernels, but the most popular one is the Gaussian kernel. Adding up all of the individual kernels generates a probability surface example density function. Depending on the kernel bandwidth parameter used, the resultant density function will vary. Below is the KDE surface for our points above using a Gaussian kernel with a kernel bandwidth of 2.\nSurface plot:\nContour plot:\nBelow is the Python implementation :\nTry Code hereOutput:\nTo illustrate, suppose we are given a data set {ui} of points in d-dimensional space, sampled from some larger population, and that we have chosen a kernel K having bandwidth parameter h. Together, these data and kernel function returns the following kernel density estimator for the full populationâ€™s density function.\nThe kernel function here is required to satisfy the following two conditions:\nTwo popular kernel functions that satisfy these conditions are given by-\nBelow we plot an example in one dimension using the Gaussian kernel to estimate the density of some population along the x-axis. We can see that each sample point adds a small Gaussian to our estimate, centered about it and equations above may look a bit intimidating, but the graphic here should clarify that the concept is pretty straightforward.\nIterative Mode Search -\nGeneral algorithm outline -\nShift function looks like this -\nPros:\nFinds variable number of modes\nRobust to outliers\nGeneral, application-independent tool\nModel-free, doesn't assume any prior shape like spherical, elliptical, etc. on data clusters\nJust a single parameter (window size h) where h has a physical meaning (unlike k-means)\nCons:\nOutput depends on window size\nWindow size (bandwidth) selecHon is not trivial\nComputationally (relatively) expensive (approx 2s/image)\nDoesn't scale well with dimension of feature space."
  }
]