[
  {
    "input": "Implementation of Types of Linear Regression",
    "output": "We will discuss three types of linear regression:\nSimple linear regression:This involves predicting a dependent variable based on a single independent variable.\nMultiple linear regression:This involves predicting a dependent variable based on multiple independent variables.\nPolynomial linear regression:This involves predicting a dependent variable based on a polynomial relationship between independent and dependent variables."
  },
  {
    "input": "1. Simple Linear Regression",
    "output": "Simple linear regression is an approach for predicting aresponseusing asingle feature. It is one of the most basic and simple machine learning models. In linear regression we assume that the two variables i.e. dependent and independent variables are linearly related. Hence we try to find a linear function that predicts the value (y)  with reference to independent variable(x). Let us consider a dataset where we have a value of response y for every feature x:\nFor generality, we define:\nfornobservations (in the above example, n=10). A scatter plot of the above dataset looks like this:-\nNow, the task is to find aline that fits bestin the above scatter plot so that we can predict the response for any new feature values. (i.e a value of x not present in a dataset) This line is called aregression line. The equation of the regression line is represented as:\nh(x_i) = \\beta _0 + \\beta_1x_i\nHere,\nh(x_i) represents thepredicted response valuefor ithobservation.\nb_0 and b_1 are regression coefficients and represent they-interceptandslopeof the regression line respectively.\nTo create our model we must \"learn\" or estimate the values of regression coefficients b_0 and b_1. And once we've estimated these coefficients, we can use the model to predict responses!In this article we are going to use the principle ofLeast Squares.\nNow consider:\ny_i = \\beta_0 + \\beta_1x_i + \\varepsilon_i = h(x_i) + \\varepsilon_i \\Rightarrow \\varepsilon_i = y_i -h(x_i)\nHere, e_i is aresidual errorin ith observation. So, our aim is to minimize the total residual error. We define the squared error or cost function, J as:\nJ(\\beta_0,\\beta_1)= \\frac{1}{2n} \\sum_{i=1}^{n} \\varepsilon_i^{2}\nAnd our task is to find the value of b0and b1for which J(b0, b1) is minimum! Without going into the mathematical details, we present the result here:\n\\beta_1 = \\frac{SS_{xy}}{SS_{xx}}\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\nWhere SSxyis the sum of cross-deviations of y and x:\nSS_{xy} = \\sum_{i=1}^{n} (x_i-\\bar{x})(y_i-\\bar{y}) = \\sum_{i=1}^{n} y_ix_i - n\\bar{x}\\bar{y}\nAnd SSxxis the sum of squared deviations of x:\nSS_{xx} = \\sum_{i=1}^{n} (x_i-\\bar{x})^2 = \\sum_{i=1}^{n}x_i^2 - n(\\bar{x})^2"
  },
  {
    "input": "Python Implementation of Simple Linear Regression",
    "output": "We can use the Python language to learn the coefficient of linear regression models. For plotting the input data and best-fitted line we will use the matplotlib library. It is one of the most used Python libraries for plotting graphs. Here is the example of simpe Linear regression using Python.\nThis functionestimate_coef(), takes the input datax(independent variable) andy(dependent variable) and estimates the coefficients of the linear regression line using the least squares method.\nCalculating Number of Observations:n = np.size(x)determines the number of data points.\nCalculating Means:m_x = np.mean(x)andm_y = np.mean(y)compute the mean values ofxandy, respectively.\nCalculating Cross-Deviation and Deviation about x:SS_xy = np.sum(y*x) - n*m_y*m_xandSS_xx = np.sum(x*x) - n*m_x*m_xcalculate the sum of squared deviations betweenxandyand the sum of squared deviations ofxabout its mean, respectively.\nCalculating Regression Coefficients:b_1 = SS_xy / SS_xxandb_0 = m_y - b_1*m_xdetermine the slope (b_1) and intercept (b_0) of the regression line using the least squares method.\nReturning Coefficients:The function returns the estimated coefficients as a tuple(b_0, b_1).\nThis functionplot_regression_line(), takes the input datax(independent variable),y(dependent variable) and the estimated coefficientsbto plot the regression line and the data points.\nOutput:\nThe provided code implements simple linear regression analysis by defining a functionmain()that performs the following steps:\nOutput:"
  },
  {
    "input": "2. Multiple Linear Regression",
    "output": "Multiple linear regression attempts to model the relationship betweentwo or more featuresand a response by fitting a linear equation to the observed data. It is a extension of simple linear regression. Consider a dataset withpfeatures(or independent variables) and one response(or dependent variable).Also, the dataset containsnrows/observations.\nWe define:\nX (feature matrix) = a matrix of sizen X pwhere xijdenotes the values of the jthfeature for ith observation.\nSo,\n\\begin{pmatrix} x_{11} & \\cdots & x_{1p} \\\\ x_{21} & \\cdots & x_{2p} \\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\vdots & x_{np} \\end{pmatrix}\nand\ny (response vector) = a vector of sizenwhere y_{i} denotes the value of response for ith observation.\ny = \\begin{bmatrix} y_1\\\\ y_2\\\\ .\\\\ .\\\\ y_n \\end{bmatrix}\nTheregression lineforpfeatures is represented as:\nh(x_i) = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + .... + \\beta_px_{ip}\nwhere h(x_i) ispredicted response valuefor ith observation and b_0, b_1, ..., b_p are theregression coefficients. Also, we can write:\n\\newline y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + .... + \\beta_px_{ip} + \\varepsilon_i \\newline or \\newline y_i = h(x_i) + \\varepsilon_i \\Rightarrow \\varepsilon_i = y_i - h(x_i)\nwhere e_i represents aresidual errorin ith observation. We can generalize our linear model a little bit more by representing feature matrixXas:\nX = \\begin{pmatrix} 1 & x_{11} & \\cdots & x_{1p} \\\\ 1 & x_{21} & \\cdots & x_{2p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{n1} & \\cdots & x_{np} \\end{pmatrix}\nSo now, the linear model can be expressed in terms of matrices as:\ny = X\\beta + \\varepsilon\nwhere,\n\\beta = \\begin{bmatrix} \\beta_0\\\\ \\beta_1\\\\ .\\\\ .\\\\ \\beta_p \\end{bmatrix}\nand\n\\varepsilon = \\begin{bmatrix} \\varepsilon_1\\\\ \\varepsilon_2\\\\ .\\\\ .\\\\ \\varepsilon_n \\end{bmatrix}\nNow, we determine anestimate of bi.e. b' using theLeast Squares method. As already explained, the Least Squares method tends to determine b' for which total residual error is minimized.We present the result directly here:\n\\hat{\\beta} = ({X}'X)^{-1} {X}'y\nwhere ' represents the transpose of the matrix while -1 represents thematrix inverse. Knowing the least square estimates, b', the multiple linear regression model can now be estimated as:\n\\hat{y} = X\\hat{\\beta}\nwhere y' is theestimated response vector."
  },
  {
    "input": "Python Implementation of Multiple Linear Regression",
    "output": "For multiple linear regression using Python, we will use theBoston house pricing dataset.\nThe code downloads the Boston Housing dataset from the provided URL and reads it into a Pandas DataFrame (raw_df)\nThis extracts the input variables (X) and target variable (y) from the DataFrame. The input variables are selected from every other row to match the target variable, which is available every other row.\nHere it divides the data into training and testing sets using thetrain_test_split()function from scikit-learn. Thetest_sizeparameter specifies that 40% of the data should be used for testing.\nThis initializes a LinearRegression object (reg) and trains the model using the training data (X_train,y_train)\nEvaluates the model's performance by printing the regression coefficients and calculating the variance score, which measures the proportion of explained variance. A score of 1 indicates perfect prediction.\nOutput:\nPlotting Residual Errors\nPlotting and analyzing the residual errors, which represent the difference between the predicted values and the actual values.\nOutput:\nIn the above example, we determine the accuracy score usingExplained Variance Score. We define:\nexplained_variance_score = 1 - Var{y - y'}/Var{y}\nwhere y' is the estimated target output, y is the corresponding (correct) target output, and Var is Variance, the square of the standard deviation. The best possible score is 1.0, lower values are worse."
  },
  {
    "input": "3. Polynomial Linear Regression",
    "output": "Polynomial Regressionis a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as annth-degreepolynomial. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y | x).\nThe choice of degree for polynomial regression is a trade-off between bias and variance. Bias is the tendency of a model to consistently predict the same value, regardless of the true value of the dependent variable. Variance is the tendency of a model to make different predictions for the same data point, depending on the specific training data used.\nA higher-degree polynomial can reduce bias but can also increase variance, leading to overfitting. Conversely, a lower-degree polynomial can reduce variance but can also increase bias.\nThere are a number of methods for choosing a degree for polynomial regression, such as cross-validation and using information criteria such as Akaike information criterion (AIC) or Bayesian information criterion (BIC)."
  },
  {
    "input": "Implementation of Polynomial Regression using Python",
    "output": "Implementing the Polynomial regression using Python:\nHere we will import all the necessary libraries for data analysis and machine learning tasks and then loads the 'Position_Salaries.csv' dataset using Pandas. It then prepares the data for modeling by handling missing values and encoding categorical data. Finally, it splits the data into training and testing sets and standardizes the numerical features using StandardScaler.\nOutput:\nThe code creates a linear regression model and fits it to the provided data, establishing a linear relationship between the independent and dependent variables.\nThe code performs quadratic and cubic regression by generating polynomial features from the original data and fitting linear regression models to these features. This enables modeling nonlinear relationships between the independent and dependent variables.\nThe code creates a scatter plot of the data point, It effectively visualizes the linear relationship between position level and salary.\nOutput:\n\nThe code creates a scatter plot of the data points, overlays the predicted quadratic and cubic regression lines. It effectively visualizes the nonlinear relationship between position level and salary and compares the fits of quadratic and cubic regression models.\nOutput:\n\nThe code effectively visualizes the relationship between position level and salary using cubic regression and generates a continuous prediction line for a broader range of position levels.\nOutput:"
  }
]