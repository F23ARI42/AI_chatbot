[
  {
    "input": "RNN Architecture",
    "output": "At each timestept, the RNN maintains a hidden stateS_t​, which acts as the network’s memory summarizing information from previous inputs. The hidden stateS_t​ updates by combining the current inputX_t​ and the previous hidden stateS_{t-1}, applying an activation function to introduce non-linearity. Then the outputY_t​is generated by transforming this hidden state.\nS_t = g_1(W_x X_t + W_s S_{t-1})\nS_trepresents the hidden state (memory) at timet.\nX_t​ is the input at timet.\nY_t​ is the output at timet.\nW_s, W_x, W_y​ are weight matrices for hidden states, inputs and outputs, respectively.\nY_t = g_2(W_y S_t)\nwhereg_1​ andg_2​ are activation functions."
  },
  {
    "input": "Error Function at Timet=3",
    "output": "To train the network, we measure how far the predicted outputY_t​ is from the desired outputd_t​ using an error function. We use the squared error to measure the difference between the desired outputd_tand actual outputY_t:\nE_t = (d_t - Y_t)^2\nAtt=3:\nE_3 = (d_3 - Y_3)^2\nThis error quantifies the difference between the predicted output and the actual output at time 3."
  },
  {
    "input": "Updating Weights Using BPTT",
    "output": "BPTT updates the weightsW_y, W_s, W_xto minimize the error by computing gradients. Unlike standard backpropagation, BPTT unfolds the network across time steps, considering how errors at timetdepend on all previous states.\nWe want to adjust the weightsW_y​,W_s​ andW_x​ to minimize the errorE_3​."
  },
  {
    "input": "1. Adjusting Output WeightW_y",
    "output": "The output weightW_y​ affects the output directly at time 3. This means we calculate how the error changes asY_3​ changes, then howY_3​ changes with respect toW_y​. UpdatingW_y​ is straightforward because it only influences the current output.\nUsing the chain rule:\n\\frac{\\partial E_3}{\\partial W_y} = \\frac{\\partial E_3}{\\partial Y_3} \\times \\frac{\\partial Y_3}{\\partial W_y}\nE_3depends onY_3​, so we differentiateE_3​ w.r.t.Y_3​.\nY_3​ depends onW_y​, so we differentiateY_3​ w.r.t.W_y​."
  },
  {
    "input": "2. Adjusting Hidden State WeightW_s",
    "output": "The hidden state weightW_s​ influences not just the current hidden state but all previous ones because each hidden state depends on the previous one. To updateW_s​, we must consider how changes toW_s​ affect all hidden statesS_1, S_2, S_3and consequently the output at time 3.\nThe gradient forW_s​ considers all previous hidden states because each hidden state depends on the previous one:\n\\frac{\\partial E_3}{\\partial W_s} = \\sum_{i=1}^3 \\frac{\\partial E_3}{\\partial Y_3} \\times \\frac{\\partial Y_3}{\\partial S_i} \\times \\frac{\\partial S_i}{\\partial W_s}\nBreaking down:\nStart with the error gradient at outputY_3​.\nPropagate gradients back through all hidden statesS_3, S_2, S_1since they affectY_3​.\nEachS_i​ depends onW_s​, so we differentiate accordingly.Adjusting Ws"
  },
  {
    "input": "3. Adjusting Input WeightW_x​",
    "output": "Similar toW_s​, the input weightW_x​ affects all hidden states because the input at each timestep shapes the hidden state. The process considers how every input in the sequence impacts the hidden states leading to the output at time 3.\n\\frac{\\partial E_3}{\\partial W_x} = \\sum_{i=1}^3 \\frac{\\partial E_3}{\\partial Y_3} \\times \\frac{\\partial Y_3}{\\partial S_i} \\times \\frac{\\partial S_i}{\\partial W_x}\nThe process is similar toW_s​, accounting for all previous hidden states because inputs at each timestep affect the hidden states."
  },
  {
    "input": "Advantages of Backpropagation Through Time (BPTT)",
    "output": "Captures Temporal Dependencies:BPTT allows RNNs to learn relationships across time steps, crucial for sequential data like speech, text and time series.\nUnfolding over Time:By considering all previous states during training, BPTT helps the model understand how past inputs influence future outputs.\nFoundation for Modern RNNs:BPTT forms the basis for training advanced architectures such as LSTMs and GRUs, enabling effective learning of long sequences.\nFlexible for Variable Length Sequences: It can handle input sequences of varying lengths, adapting gradient calculations accordingly."
  },
  {
    "input": "Limitations of BPTT",
    "output": "Vanishing Gradient Problem:When backpropagating over many time steps, gradients tend to shrink exponentially, making early time steps contribute very little to weight updates. This causes the network to “forget” long-term dependencies.\nExploding Gradient Problem:Gradients may also grow uncontrollably large, causing unstable updates and making training difficult."
  },
  {
    "input": "Solutions",
    "output": "Long Short-Term Memory (LSTM):Special RNN cells designed to maintain information over longer sequences and mitigate vanishing gradients.\nGradient Clipping:Limits the magnitude of gradients during backpropagation to prevent explosion by normalizing them when exceeding a threshold.\nIn this article, we learned how Backpropagation Through Time (BPTT) enables Recurrent Neural Networks to capture temporal dependencies by updating weights across multiple time steps along with its challenges and solutions."
  }
]